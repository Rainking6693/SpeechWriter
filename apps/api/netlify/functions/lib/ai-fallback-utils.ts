import OpenAI from 'openai'\nimport Anthropic from '@anthropic-ai/sdk'\nimport { drizzle } from 'drizzle-orm/postgres-js'\nimport { eq } from 'drizzle-orm'\nimport postgres from 'postgres'\nimport { modelRuns } from '@speechwriter/database/schema'\n\nconst client = postgres(process.env.DATABASE_URL!, { prepare: false })\nconst db = drizzle(client, { schema: { modelRuns } })\n\n// Initialize AI clients\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY!,\n})\n\nconst anthropic = new Anthropic({\n  apiKey: process.env.ANTHROPIC_API_KEY!,\n})\n\ninterface AICallOptions {\n  model?: string\n  temperature?: number\n  maxTokens?: number\n  timeout?: number\n  retries?: number\n  fallbackProvider?: 'openai' | 'anthropic' | 'mock'\n}\n\ninterface AIResponse {\n  content: string\n  provider: 'openai' | 'anthropic' | 'mock'\n  model: string\n  tokensUsed?: number\n  success: boolean\n  error?: string\n}\n\n/**\n * Robust AI call with fallback logic\n */\nexport async function robustAICall(\n  messages: Array<{ role: string; content: string }>,\n  options: AICallOptions = {},\n  modelRunId?: string\n): Promise<AIResponse> {\n  const {\n    model = 'gpt-4',\n    temperature = 0.7,\n    maxTokens = 2000,\n    timeout = 30000, // 30 seconds\n    retries = 2,\n    fallbackProvider = 'anthropic'\n  } = options\n\n  let lastError: Error | null = null\n  \n  // Try primary provider (OpenAI) with retries\n  for (let attempt = 1; attempt <= retries; attempt++) {\n    try {\n      console.log(`Attempting OpenAI call (attempt ${attempt}/${retries})`)\n      \n      const response = await Promise.race([\n        openai.chat.completions.create({\n          model,\n          messages: messages as any,\n          temperature,\n          max_tokens: maxTokens,\n        }),\n        new Promise((_, reject) => \n          setTimeout(() => reject(new Error('OpenAI timeout')), timeout)\n        )\n      ]) as OpenAI.Chat.Completions.ChatCompletion\n\n      const content = response.choices[0]?.message?.content || ''\n      \n      if (content.trim()) {\n        // Update model run with success\n        if (modelRunId) {\n          await updateModelRun(modelRunId, {\n            provider: 'openai',\n            model,\n            outputTokens: response.usage?.completion_tokens,\n            totalTokens: response.usage?.total_tokens,\n            success: true\n          })\n        }\n\n        return {\n          content,\n          provider: 'openai',\n          model,\n          tokensUsed: response.usage?.total_tokens,\n          success: true\n        }\n      }\n    } catch (error) {\n      lastError = error as Error\n      console.error(`OpenAI attempt ${attempt} failed:`, error.message)\n      \n      // If it's a rate limit error, wait before retrying\n      if (error.message.includes('rate_limit') && attempt < retries) {\n        const waitTime = Math.pow(2, attempt) * 1000 // Exponential backoff\n        console.log(`Rate limited, waiting ${waitTime}ms before retry`)\n        await new Promise(resolve => setTimeout(resolve, waitTime))\n      }\n    }\n  }\n\n  // Try fallback provider (Anthropic)\n  if (fallbackProvider === 'anthropic' && process.env.ANTHROPIC_API_KEY) {\n    try {\n      console.log('Trying Anthropic fallback')\n      \n      // Convert OpenAI format to Anthropic format\n      const anthropicMessages = messages.filter(m => m.role !== 'system')\n      const systemMessage = messages.find(m => m.role === 'system')?.content || ''\n      \n      const response = await Promise.race([\n        anthropic.messages.create({\n          model: 'claude-3-sonnet-20240229',\n          max_tokens: maxTokens,\n          temperature,\n          system: systemMessage,\n          messages: anthropicMessages as any,\n        }),\n        new Promise((_, reject) => \n          setTimeout(() => reject(new Error('Anthropic timeout')), timeout)\n        )\n      ]) as any\n\n      const content = response.content[0]?.text || ''\n      \n      if (content.trim()) {\n        // Update model run with fallback success\n        if (modelRunId) {\n          await updateModelRun(modelRunId, {\n            provider: 'anthropic',\n            model: 'claude-3-sonnet-20240229',\n            outputTokens: response.usage?.output_tokens,\n            totalTokens: response.usage?.input_tokens + response.usage?.output_tokens,\n            success: true,\n            fallbackUsed: true\n          })\n        }\n\n        return {\n          content,\n          provider: 'anthropic',\n          model: 'claude-3-sonnet-20240229',\n          tokensUsed: response.usage?.input_tokens + response.usage?.output_tokens,\n          success: true\n        }\n      }\n    } catch (error) {\n      console.error('Anthropic fallback failed:', error.message)\n      lastError = error as Error\n    }\n  }\n\n  // Final fallback: Mock response\n  console.log('All AI providers failed, using mock response')\n  \n  const mockContent = generateMockResponse(messages)\n  \n  // Update model run with mock fallback\n  if (modelRunId) {\n    await updateModelRun(modelRunId, {\n      provider: 'mock',\n      model: 'fallback',\n      success: false,\n      error: lastError?.message || 'All AI providers failed',\n      fallbackUsed: true\n    })\n  }\n\n  return {\n    content: mockContent,\n    provider: 'mock',\n    model: 'fallback',\n    success: false,\n    error: lastError?.message || 'All AI providers failed'\n  }\n}\n\n/**\n * Generate a mock response when all AI providers fail\n */\nfunction generateMockResponse(messages: Array<{ role: string; content: string }>): string {\n  const userMessage = messages.find(m => m.role === 'user')?.content || ''\n  \n  // Detect the type of request and provide appropriate fallback\n  if (userMessage.toLowerCase().includes('outline')) {\n    return JSON.stringify({\n      sections: [\n        {\n          title: \"Opening\",\n          description: \"Engaging introduction to capture audience attention\",\n          allocatedTimeMinutes: 2,\n          sectionType: \"opening\",\n          keyPoints: [\"Hook the audience\", \"Introduce main theme\"],\n          notes: \"Start with a compelling story or question\"\n        },\n        {\n          title: \"Main Content\",\n          description: \"Core message and supporting points\",\n          allocatedTimeMinutes: 10,\n          sectionType: \"body\",\n          keyPoints: [\"Present main argument\", \"Provide supporting evidence\"],\n          notes: \"Use clear examples and logical flow\"\n        },\n        {\n          title: \"Conclusion\",\n          description: \"Memorable closing with call to action\",\n          allocatedTimeMinutes: 3,\n          sectionType: \"close\",\n          keyPoints: [\"Summarize key points\", \"End with impact\"],\n          notes: \"Leave audience with something to remember\"\n        }\n      ],\n      totalDuration: 15,\n      callbackOpportunities: [\"Reference opening story in conclusion\"],\n      quotableMoments: [\"A memorable closing line that resonates\"]\n    })\n  }\n  \n  if (userMessage.toLowerCase().includes('draft') || userMessage.toLowerCase().includes('section')) {\n    return `I apologize, but our AI services are temporarily unavailable. Here's a placeholder for this section:\n\n[PAUSE]\n\nThis section would contain compelling content tailored to your audience and occasion. [EMPHASIZE] The key message would be clearly articulated with supporting examples and stories.\n\n[PAUSE]\n\nPlease try regenerating this section in a few minutes when our services are restored, or contact support if the issue persists.\n\n[EMPHASIZE] Thank you for your patience.`\n  }\n  \n  // Generic fallback\n  return `I apologize, but our AI services are temporarily unavailable. Please try again in a few minutes, or contact support if the issue persists. We're working to restore full functionality as quickly as possible.`\n}\n\n/**\n * Update model run record with results\n */\nasync function updateModelRun(modelRunId: string, data: {\n  provider: string\n  model: string\n  outputTokens?: number\n  totalTokens?: number\n  success: boolean\n  error?: string\n  fallbackUsed?: boolean\n}) {\n  try {\n    await db\n      .update(modelRuns)\n      .set({\n        provider: data.provider,\n        model: data.model,\n        outputTokens: data.outputTokens || 0,\n        totalTokens: data.totalTokens || 0,\n        success: data.success,\n        error: data.error || null,\n        metadata: {\n          fallback_used: data.fallbackUsed || false,\n          completed_at: new Date().toISOString()\n        },\n        updatedAt: new Date()\n      })\n      .where(eq(modelRuns.id, modelRunId))\n  } catch (error) {\n    console.error('Failed to update model run:', error)\n  }\n}\n\n/**\n * Check if AI services are healthy\n */\nexport async function checkAIHealth(): Promise<{\n  openai: boolean\n  anthropic: boolean\n  overall: boolean\n}> {\n  const results = {\n    openai: false,\n    anthropic: false,\n    overall: false\n  }\n\n  // Test OpenAI\n  try {\n    await openai.chat.completions.create({\n      model: 'gpt-3.5-turbo',\n      messages: [{ role: 'user', content: 'Test' }],\n      max_tokens: 5\n    })\n    results.openai = true\n  } catch (error) {\n    console.error('OpenAI health check failed:', error.message)\n  }\n\n  // Test Anthropic\n  if (process.env.ANTHROPIC_API_KEY) {\n    try {\n      await anthropic.messages.create({\n        model: 'claude-3-haiku-20240307',\n        max_tokens: 5,\n        messages: [{ role: 'user', content: 'Test' }]\n      })\n      results.anthropic = true\n    } catch (error) {\n      console.error('Anthropic health check failed:', error.message)\n    }\n  }\n\n  results.overall = results.openai || results.anthropic\n  return results\n}\n\n/**\n * Get current AI rate limits and usage\n */\nexport async function getAIUsageStats(userId: string): Promise<{\n  requestsToday: number\n  tokensToday: number\n  rateLimitReached: boolean\n}> {\n  try {\n    const today = new Date()\n    today.setHours(0, 0, 0, 0)\n    \n    const runs = await db\n      .select()\n      .from(modelRuns)\n      .where(eq(modelRuns.userId, userId))\n    \n    const todayRuns = runs.filter(run => \n      new Date(run.createdAt) >= today\n    )\n    \n    const requestsToday = todayRuns.length\n    const tokensToday = todayRuns.reduce((sum, run) => sum + (run.totalTokens || 0), 0)\n    \n    // Check against rate limits (configurable via env vars)\n    const maxRequestsPerDay = parseInt(process.env.AI_RATE_LIMIT_RPD || '100')\n    const maxTokensPerDay = parseInt(process.env.AI_RATE_LIMIT_TPD || '100000')\n    \n    const rateLimitReached = requestsToday >= maxRequestsPerDay || tokensToday >= maxTokensPerDay\n    \n    return {\n      requestsToday,\n      tokensToday,\n      rateLimitReached\n    }\n  } catch (error) {\n    console.error('Error getting AI usage stats:', error)\n    return {\n      requestsToday: 0,\n      tokensToday: 0,\n      rateLimitReached: false\n    }\n  }\n}"