import { drizzle } from 'drizzle-orm/postgres-js'\nimport postgres from 'postgres'\nimport { modelRuns, performanceMetrics } from '@speechwriter/database/schema'\nimport { v4 as uuidv4 } from 'uuid'\n\nconst client = postgres(process.env.DATABASE_URL!, { prepare: false })\nconst db = drizzle(client, { schema: { modelRuns, performanceMetrics } })\n\n// In-memory cache for frequently accessed data\nconst cache = new Map<string, { data: any; timestamp: number; ttl: number }>()\n\n// Redis client (if available)\nlet redisClient: any = null\nif (process.env.REDIS_URL) {\n  try {\n    // In production, you would import and configure Redis\n    // const Redis = require('ioredis')\n    // redisClient = new Redis(process.env.REDIS_URL)\n    console.log('Redis would be initialized here')\n  } catch (error) {\n    console.warn('Redis not available, using in-memory cache')\n  }\n}\n\ninterface CacheOptions {\n  ttl?: number // Time to live in seconds\n  useRedis?: boolean\n}\n\ninterface PerformanceMetric {\n  functionName: string\n  duration: number\n  memoryUsage?: number\n  cacheHit?: boolean\n  userId?: string\n  metadata?: Record<string, any>\n}\n\n/**\n * High-performance caching utility\n */\nexport class CacheManager {\n  /**\n   * Get cached data\n   */\n  static async get<T>(key: string, options: CacheOptions = {}): Promise<T | null> {\n    const { useRedis = !!redisClient } = options\n    \n    try {\n      // Try Redis first if available\n      if (useRedis && redisClient) {\n        const cached = await redisClient.get(key)\n        if (cached) {\n          return JSON.parse(cached)\n        }\n      }\n      \n      // Fallback to in-memory cache\n      const cached = cache.get(key)\n      if (cached) {\n        const now = Date.now()\n        if (now - cached.timestamp < cached.ttl * 1000) {\n          return cached.data\n        } else {\n          cache.delete(key)\n        }\n      }\n      \n      return null\n    } catch (error) {\n      console.error('Cache get error:', error)\n      return null\n    }\n  }\n  \n  /**\n   * Set cached data\n   */\n  static async set<T>(key: string, data: T, options: CacheOptions = {}): Promise<void> {\n    const { ttl = 3600, useRedis = !!redisClient } = options // Default 1 hour TTL\n    \n    try {\n      // Set in Redis if available\n      if (useRedis && redisClient) {\n        await redisClient.setex(key, ttl, JSON.stringify(data))\n      }\n      \n      // Always set in memory cache as fallback\n      cache.set(key, {\n        data,\n        timestamp: Date.now(),\n        ttl\n      })\n    } catch (error) {\n      console.error('Cache set error:', error)\n    }\n  }\n  \n  /**\n   * Delete cached data\n   */\n  static async delete(key: string): Promise<void> {\n    try {\n      if (redisClient) {\n        await redisClient.del(key)\n      }\n      cache.delete(key)\n    } catch (error) {\n      console.error('Cache delete error:', error)\n    }\n  }\n  \n  /**\n   * Clear all cache\n   */\n  static async clear(): Promise<void> {\n    try {\n      if (redisClient) {\n        await redisClient.flushall()\n      }\n      cache.clear()\n    } catch (error) {\n      console.error('Cache clear error:', error)\n    }\n  }\n  \n  /**\n   * Get cache statistics\n   */\n  static getStats() {\n    return {\n      memoryCache: {\n        size: cache.size,\n        keys: Array.from(cache.keys())\n      },\n      redis: {\n        connected: !!redisClient,\n        url: process.env.REDIS_URL ? 'configured' : 'not configured'\n      }\n    }\n  }\n}\n\n/**\n * Performance monitoring and optimization\n */\nexport class PerformanceMonitor {\n  private static metrics: PerformanceMetric[] = []\n  \n  /**\n   * Start performance measurement\n   */\n  static startMeasurement(functionName: string) {\n    const startTime = process.hrtime.bigint()\n    const startMemory = process.memoryUsage()\n    \n    return {\n      functionName,\n      startTime,\n      startMemory,\n      end: (metadata?: Record<string, any>) => {\n        const endTime = process.hrtime.bigint()\n        const endMemory = process.memoryUsage()\n        \n        const duration = Number(endTime - startTime) / 1000000 // Convert to milliseconds\n        const memoryUsage = endMemory.heapUsed - startMemory.heapUsed\n        \n        const metric: PerformanceMetric = {\n          functionName,\n          duration,\n          memoryUsage,\n          metadata\n        }\n        \n        this.recordMetric(metric)\n        return metric\n      }\n    }\n  }\n  \n  /**\n   * Record performance metric\n   */\n  static async recordMetric(metric: PerformanceMetric) {\n    try {\n      // Store in memory for immediate access\n      this.metrics.push(metric)\n      \n      // Keep only last 1000 metrics in memory\n      if (this.metrics.length > 1000) {\n        this.metrics = this.metrics.slice(-1000)\n      }\n      \n      // Store in database for long-term analysis\n      await db.insert(performanceMetrics).values({\n        id: uuidv4(),\n        functionName: metric.functionName,\n        duration: metric.duration,\n        memoryUsage: metric.memoryUsage || 0,\n        cacheHit: metric.cacheHit || false,\n        userId: metric.userId || null,\n        metadata: metric.metadata || null,\n        timestamp: new Date()\n      })\n    } catch (error) {\n      console.error('Failed to record performance metric:', error)\n    }\n  }\n  \n  /**\n   * Get performance statistics\n   */\n  static getStats(functionName?: string) {\n    const relevantMetrics = functionName \n      ? this.metrics.filter(m => m.functionName === functionName)\n      : this.metrics\n    \n    if (relevantMetrics.length === 0) {\n      return null\n    }\n    \n    const durations = relevantMetrics.map(m => m.duration)\n    const memoryUsages = relevantMetrics.map(m => m.memoryUsage || 0)\n    \n    return {\n      count: relevantMetrics.length,\n      duration: {\n        min: Math.min(...durations),\n        max: Math.max(...durations),\n        avg: durations.reduce((a, b) => a + b, 0) / durations.length,\n        p95: this.percentile(durations, 95),\n        p99: this.percentile(durations, 99)\n      },\n      memory: {\n        min: Math.min(...memoryUsages),\n        max: Math.max(...memoryUsages),\n        avg: memoryUsages.reduce((a, b) => a + b, 0) / memoryUsages.length\n      },\n      cacheHitRate: relevantMetrics.filter(m => m.cacheHit).length / relevantMetrics.length\n    }\n  }\n  \n  /**\n   * Calculate percentile\n   */\n  private static percentile(arr: number[], p: number): number {\n    const sorted = arr.slice().sort((a, b) => a - b)\n    const index = (p / 100) * (sorted.length - 1)\n    const lower = Math.floor(index)\n    const upper = Math.ceil(index)\n    const weight = index % 1\n    \n    return sorted[lower] * (1 - weight) + sorted[upper] * weight\n  }\n}\n\n/**\n * Optimized database query utilities\n */\nexport class QueryOptimizer {\n  /**\n   * Batch database operations\n   */\n  static async batchInsert<T>(table: any, records: T[], batchSize = 100) {\n    const results = []\n    \n    for (let i = 0; i < records.length; i += batchSize) {\n      const batch = records.slice(i, i + batchSize)\n      const result = await db.insert(table).values(batch).returning()\n      results.push(...result)\n    }\n    \n    return results\n  }\n  \n  /**\n   * Cached query with automatic invalidation\n   */\n  static async cachedQuery<T>(\n    cacheKey: string,\n    queryFn: () => Promise<T>,\n    options: CacheOptions = {}\n  ): Promise<T> {\n    const measurement = PerformanceMonitor.startMeasurement('cached_query')\n    \n    // Try cache first\n    const cached = await CacheManager.get<T>(cacheKey, options)\n    if (cached !== null) {\n      measurement.end({ cache_hit: true, cache_key: cacheKey })\n      return cached\n    }\n    \n    // Execute query\n    const result = await queryFn()\n    \n    // Cache result\n    await CacheManager.set(cacheKey, result, options)\n    \n    measurement.end({ cache_hit: false, cache_key: cacheKey })\n    return result\n  }\n  \n  /**\n   * Paginated query with caching\n   */\n  static async paginatedQuery<T>(\n    queryFn: (offset: number, limit: number) => Promise<T[]>,\n    page: number = 1,\n    pageSize: number = 20,\n    cachePrefix?: string\n  ) {\n    const offset = (page - 1) * pageSize\n    const cacheKey = cachePrefix ? `${cachePrefix}:page:${page}:size:${pageSize}` : null\n    \n    if (cacheKey) {\n      return this.cachedQuery(\n        cacheKey,\n        () => queryFn(offset, pageSize),\n        { ttl: 300 } // 5 minutes for paginated results\n      )\n    }\n    \n    return queryFn(offset, pageSize)\n  }\n}\n\n/**\n * Function optimization utilities\n */\nexport function withPerformanceMonitoring<T extends any[], R>(\n  fn: (...args: T) => Promise<R>,\n  functionName: string\n) {\n  return async (...args: T): Promise<R> => {\n    const measurement = PerformanceMonitor.startMeasurement(functionName)\n    \n    try {\n      const result = await fn(...args)\n      measurement.end({ success: true })\n      return result\n    } catch (error) {\n      measurement.end({ success: false, error: error.message })\n      throw error\n    }\n  }\n}\n\n/**\n * Debounce function calls\n */\nexport function debounce<T extends any[]>(\n  fn: (...args: T) => void,\n  delay: number\n) {\n  let timeoutId: NodeJS.Timeout\n  \n  return (...args: T) => {\n    clearTimeout(timeoutId)\n    timeoutId = setTimeout(() => fn(...args), delay)\n  }\n}\n\n/**\n * Throttle function calls\n */\nexport function throttle<T extends any[]>(\n  fn: (...args: T) => void,\n  limit: number\n) {\n  let inThrottle: boolean\n  \n  return (...args: T) => {\n    if (!inThrottle) {\n      fn(...args)\n      inThrottle = true\n      setTimeout(() => inThrottle = false, limit)\n    }\n  }\n}\n\n/**\n * Memory usage monitoring\n */\nexport class MemoryMonitor {\n  private static readonly WARNING_THRESHOLD = 0.8 // 80% of heap limit\n  private static readonly CRITICAL_THRESHOLD = 0.9 // 90% of heap limit\n  \n  /**\n   * Get current memory usage\n   */\n  static getUsage() {\n    const usage = process.memoryUsage()\n    const heapLimit = 1.4 * 1024 * 1024 * 1024 // Approximate Node.js heap limit (1.4GB)\n    \n    return {\n      heapUsed: usage.heapUsed,\n      heapTotal: usage.heapTotal,\n      heapLimit,\n      heapUsedPercentage: usage.heapUsed / heapLimit,\n      external: usage.external,\n      rss: usage.rss,\n      arrayBuffers: usage.arrayBuffers\n    }\n  }\n  \n  /**\n   * Check if memory usage is concerning\n   */\n  static checkMemoryHealth() {\n    const usage = this.getUsage()\n    \n    if (usage.heapUsedPercentage > this.CRITICAL_THRESHOLD) {\n      return {\n        status: 'critical',\n        message: `Memory usage critical: ${Math.round(usage.heapUsedPercentage * 100)}%`,\n        recommendation: 'Immediate garbage collection recommended'\n      }\n    } else if (usage.heapUsedPercentage > this.WARNING_THRESHOLD) {\n      return {\n        status: 'warning',\n        message: `Memory usage high: ${Math.round(usage.heapUsedPercentage * 100)}%`,\n        recommendation: 'Monitor closely and consider optimization'\n      }\n    }\n    \n    return {\n      status: 'healthy',\n      message: `Memory usage normal: ${Math.round(usage.heapUsedPercentage * 100)}%`,\n      recommendation: 'No action needed'\n    }\n  }\n  \n  /**\n   * Force garbage collection (if available)\n   */\n  static forceGC() {\n    if (global.gc) {\n      global.gc()\n      return true\n    }\n    return false\n  }\n}\n\n/**\n * Cold start optimization for Netlify Functions\n */\nexport class ColdStartOptimizer {\n  private static initialized = false\n  private static initPromise: Promise<void> | null = null\n  \n  /**\n   * Initialize expensive resources once\n   */\n  static async initialize() {\n    if (this.initialized) {\n      return\n    }\n    \n    if (this.initPromise) {\n      return this.initPromise\n    }\n    \n    this.initPromise = this.doInitialization()\n    await this.initPromise\n    this.initialized = true\n  }\n  \n  private static async doInitialization() {\n    // Pre-warm database connections\n    try {\n      await db.select().from(modelRuns).limit(1)\n    } catch (error) {\n      console.warn('Database pre-warm failed:', error)\n    }\n    \n    // Pre-load frequently used data\n    try {\n      // Could pre-load user personas, common stories, etc.\n      console.log('Cold start optimization completed')\n    } catch (error) {\n      console.warn('Data pre-load failed:', error)\n    }\n  }\n  \n  /**\n   * Wrapper for functions to ensure initialization\n   */\n  static withInitialization<T extends any[], R>(\n    fn: (...args: T) => Promise<R>\n  ) {\n    return async (...args: T): Promise<R> => {\n      await this.initialize()\n      return fn(...args)\n    }\n  }\n}\n\n/**\n * Bundle size optimization utilities\n */\nexport class BundleOptimizer {\n  /**\n   * Lazy load modules\n   */\n  static async lazyImport<T>(importFn: () => Promise<T>): Promise<T> {\n    try {\n      return await importFn()\n    } catch (error) {\n      console.error('Lazy import failed:', error)\n      throw error\n    }\n  }\n  \n  /**\n   * Conditional imports based on environment\n   */\n  static async conditionalImport<T>(\n    condition: boolean,\n    importFn: () => Promise<T>\n  ): Promise<T | null> {\n    if (!condition) {\n      return null\n    }\n    \n    return this.lazyImport(importFn)\n  }\n}\n\n/**\n * Export performance utilities\n */\nexport {\n  cache,\n  redisClient\n}"